{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
    "\n",
    "'''\n",
    "Defines the set of symbols used in text input to the model.\n",
    "\n",
    "The default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details. '''\n",
    "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "valid_symbols = [\n",
    "  'AA', 'AA0', 'AA1', 'AA2', 'AE', 'AE0', 'AE1', 'AE2', 'AH', 'AH0', 'AH1', 'AH2',\n",
    "  'AO', 'AO0', 'AO1', 'AO2', 'AW', 'AW0', 'AW1', 'AW2', 'AY', 'AY0', 'AY1', 'AY2',\n",
    "  'B', 'CH', 'D', 'DH', 'EH', 'EH0', 'EH1', 'EH2', 'ER', 'ER0', 'ER1', 'ER2', 'EY',\n",
    "  'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH', 'IH0', 'IH1', 'IH2', 'IY', 'IY0', 'IY1',\n",
    "  'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OW0', 'OW1', 'OW2', 'OY', 'OY0',\n",
    "  'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UH0', 'UH1', 'UH2', 'UW',\n",
    "  'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "\n",
    "_valid_symbol_set = set(valid_symbols)\n",
    "\n",
    "\n",
    "class CMUDict:\n",
    "  '''Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict'''\n",
    "  def __init__(self, file_or_path, keep_ambiguous=True):\n",
    "    if isinstance(file_or_path, str):\n",
    "      with open(file_or_path, encoding='latin-1') as f:\n",
    "        entries = _parse_cmudict(f)\n",
    "    else:\n",
    "      entries = _parse_cmudict(file_or_path)\n",
    "    if not keep_ambiguous:\n",
    "      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n",
    "    self._entries = entries\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self._entries)\n",
    "\n",
    "\n",
    "  def lookup(self, word):\n",
    "    '''Returns list of ARPAbet pronunciations of the given word.'''\n",
    "    return self._entries.get(word.upper())\n",
    "\n",
    "\n",
    "\n",
    "_alt_re = re.compile(r'\\([0-9]+\\)')\n",
    "\n",
    "\n",
    "def _parse_cmudict(file):\n",
    "  cmudict = {}\n",
    "  for line in file:\n",
    "    if len(line) and (line[0] >= 'A' and line[0] <= 'Z' or line[0] == \"'\"):\n",
    "      parts = line.split('  ')\n",
    "      word = re.sub(_alt_re, '', parts[0])\n",
    "      pronunciation = _get_pronunciation(parts[1])\n",
    "      if pronunciation:\n",
    "        if word in cmudict:\n",
    "          cmudict[word].append(pronunciation)\n",
    "        else:\n",
    "          cmudict[word] = [pronunciation]\n",
    "  return cmudict\n",
    "\n",
    "\n",
    "def _get_pronunciation(s):\n",
    "  parts = s.strip().split(' ')\n",
    "  for part in parts:\n",
    "    if part not in _valid_symbol_set:\n",
    "      return None\n",
    "  return ' '.join(parts)\n",
    "\n",
    "_pad        = '_'\n",
    "_punctuation = '!\\'(),.:;? '\n",
    "_special = '-'\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n",
    "_arpabet = ['@' + s for s in valid_symbols]\n",
    "\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet\n",
    "\n",
    "class hps:\n",
    "\t################################\n",
    "\t# Data Parameters              #\n",
    "\t################################\n",
    "\ttext_cleaners=['english_cleaners']\n",
    "\n",
    "\t################################\n",
    "\t# Audio                        #\n",
    "\t################################\n",
    "\tnum_mels = 80\n",
    "\tnum_freq = 1025\n",
    "\tsample_rate = 16000\n",
    "\tframe_length_ms = 50\n",
    "\tframe_shift_ms = 12.5\n",
    "\tpreemphasis = 0.97\n",
    "\tmin_level_db = -100\n",
    "\tref_level_db = 20\n",
    "\tpower = 1.5\n",
    "\tgl_iters = 100\n",
    "\n",
    "\t################################\n",
    "\t# Model Parameters             #\n",
    "\t################################\n",
    "\tn_symbols = len(symbols)\n",
    "\tsymbols_embedding_dim = 512\n",
    "\n",
    "\t# Encoder parameters\n",
    "\tencoder_kernel_size = 5\n",
    "\n",
    "\t# Decoder parameters\n",
    "\tn_frames_per_step = 2\n",
    "\tdecoder_rnn_dim = 1024\n",
    "\tprenet_dim = 256\n",
    "\tmax_decoder_steps = 120\n",
    "\tgate_threshold = 0.5\n",
    "\tp_attention_dropout = 0.1\n",
    "\tp_decoder_dropout = 0.1\n",
    "\n",
    "\t# Attention parameters\n",
    "\tattention_rnn_dim = 1024\n",
    "\tattention_dim = 128\n",
    "\n",
    "\t# Location Layer parameters\n",
    "\tattention_location_n_filters = 32\n",
    "\tattention_location_kernel_size = 31\n",
    "\n",
    "\t# Mel-post processing network parameters\n",
    "\tpostnet_embedding_dim = 512\n",
    "\tpostnet_kernel_size = 5\n",
    "\tpostnet_n_convolutions = 5\n",
    "\n",
    "\t################################\n",
    "\t# Train                        #\n",
    "\t################################\n",
    "\tis_cuda = False\n",
    "\tpin_mem = True\n",
    "\tn_workers = 8\n",
    "\tlr = 2e-3\n",
    "\tbetas = (0.9, 0.999)\n",
    "\teps = 1e-6\n",
    "\tsch = True\n",
    "\tsch_step = 4000\n",
    "\tmax_iter = 1e6\n",
    "\tbatch_size = 40\n",
    "\titers_per_log = 50\n",
    "\titers_per_sample = 500\n",
    "\titers_per_ckpt = 1000\n",
    "\tweight_decay = 1e-6\n",
    "\tgrad_clip_thresh = 1.0\n",
    "\tmask_padding = True\n",
    "\tp = 10 # mel spec loss penalty\n",
    "\teg_text = 'Make America great again!'\n",
    "\n",
    "\t############# added\n",
    "\tiscrop = True\n",
    "\tencoder_embedding_dim = 384  # encoder_lstm_units\n",
    "\tencoder_n_convolutions = 5  # enc_conv_num_blocks\n",
    "\n",
    "\tnum_init_filters= 24\n",
    "\n",
    "\tprenet_layers= [256, 256]\n",
    "\tdecoder_layers= 2\n",
    "\tdecoder_lstm_units= 256\n",
    "\n",
    "\ttacotron_teacher_forcing_start_decay= 29000\n",
    "\ttacotron_teacher_forcing_decay_steps= 130000\n",
    "\n",
    "\tT= 90 #90\n",
    "\toverlap= 15\n",
    "\tmel_overlap= 40\n",
    "\tmel_step_size= 240\n",
    "\timg_size = 96\n",
    "\tfps= 30\n",
    "\n",
    "\n",
    "\tuse_lws = False\n",
    "\t# Mel spectrogram\n",
    "\tn_fft = 800  # Extra window size is filled with 0 paddings to match this parameter\n",
    "\thop_size = 200  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\n",
    "\twin_size = 800  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n",
    "\n",
    "\t# M-AILABS (and other datasets) trim params (these parameters are usually correct for any\n",
    "\t# data, but definitely must be tuned for specific speakers)\n",
    "\ttrim_fft_size = 512\n",
    "\ttrim_hop_size = 128\n",
    "\ttrim_top_db = 23\n",
    "\n",
    "\t# Mel and Linear spectrograms normalization/scaling and clipping\n",
    "\tsignal_normalization = True\n",
    "\t# Whether to normalize mel spectrograms to some predefined range (following below parameters)\n",
    "\tallow_clipping_in_normalization = True # Only relevant if mel_normalization = True\n",
    "\tsymmetric_mels = True\n",
    "\t# Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,\n",
    "\t# faster and cleaner convergence)\n",
    "\tmax_abs_value = 4.\n",
    "\t# max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not\n",
    "\t# be too big to avoid gradient explosion,\n",
    "\t# not too small for fast convergence)\n",
    "\tnormalize_for_wavenet = True\n",
    "\t# whether to rescale to [0, 1] for wavenet. (better audio quality)\n",
    "\tclip_for_wavenet = True\n",
    "\t# whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n",
    "\n",
    "\t# Contribution by @begeekmyfriend\n",
    "\t# Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude\n",
    "\t# levels. Also allows for better G&L phase reconstruction)\n",
    "\tpreemphasize = True # whether to apply filter\n",
    "\n",
    "\tfmin = 55\n",
    "\t# Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To\n",
    "\t# test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n",
    "\tfmax = 7600  # To be increased/reduced depending on data.\n",
    "\n",
    "\t# Griffin Lim\n",
    "\t# Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n",
    "\tgriffin_lim_iters = 60\n",
    "# Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mode(obj, model = False):\n",
    "\tif model and hps.is_cuda:\n",
    "\t\tobj = obj\n",
    "\telif hps.is_cuda:\n",
    "\t\tobj = obj.cuda(non_blocking = hps.pin_mem)\n",
    "\treturn obj\n",
    "\n",
    "def to_var(tensor):\n",
    "\tvar = torch.autograd.Variable(tensor)\n",
    "\treturn mode(var)\n",
    "\n",
    "def to_arr(var):\n",
    "\treturn var.cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "def get_mask_from_lengths(lengths, pad = False):\n",
    "\tmax_len = torch.max(lengths).int().item()\n",
    "\tif pad and max_len%hps.n_frames_per_step != 0:\n",
    "\t\tmax_len += hps.n_frames_per_step - max_len%hps.n_frames_per_step\n",
    "\t\tassert max_len%hps.n_frames_per_step == 0\n",
    "\tids = torch.arange(0, max_len, out = torch.LongTensor(max_len))\n",
    "\tids = mode(ids)\n",
    "\tmask = (ids < lengths.unsqueeze(1))\n",
    "\treturn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal\n",
    "\n",
    "class ConvNorm3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear', activation=torch.nn.ReLU, residual=False):\n",
    "        super(ConvNorm3D, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.residual = residual\n",
    "        self.conv3d = torch.nn.Conv3d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "        self.batched = torch.nn.BatchNorm3d(out_channels)\n",
    "        self.activation = activation()\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv3d.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.batched.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.activation.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv3d(signal)\n",
    "\n",
    "        batched = self.batched(conv_signal)\n",
    "\n",
    "        if self.residual:\n",
    "            batched = batched + signal\n",
    "        activated = self.activation(batched)\n",
    "\n",
    "        return activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tacotron2Loss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Tacotron2Loss, self).__init__()\n",
    "\n",
    "\tdef forward(self, model_output, targets, iteration):\n",
    "\t\tmel_target, gate_target = targets[0], targets[1]\n",
    "\t\tmel_target.requires_grad = False\n",
    "\t\tgate_target.requires_grad = False\n",
    "\t\tslice = torch.arange(0, gate_target.size(1), hps.n_frames_per_step)\n",
    "\t\tgate_target = gate_target[:, slice].view(-1, 1)\n",
    "\n",
    "\t\tmel_out, mel_out_postnet, gate_out, _ = model_output\n",
    "\t\tgate_out = gate_out.view(-1, 1)\n",
    "\t\tp = hps.p\n",
    "\t\tmel_loss = nn.MSELoss()(p*mel_out, p*mel_target)\n",
    "\t\tmel_loss_post = nn.MSELoss()(p*mel_out_postnet, p*mel_target)\n",
    "\t\tgate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
    "\n",
    "\t\t# added\n",
    "\t\tl1_loss = nn.L1Loss()(mel_target, mel_out)\n",
    "\t\treturn mel_loss, mel_loss_post, l1_loss, gate_loss#, ((mel_loss+mel_loss_post)/(p**2)+gate_loss+l1_loss).item()\n",
    "\n",
    "\n",
    "class LocationLayer(nn.Module):\n",
    "\tdef __init__(self, attention_n_filters, attention_kernel_size,\n",
    "\t\t\t\t attention_dim):\n",
    "\t\tsuper(LocationLayer, self).__init__()\n",
    "\t\tpadding = int((attention_kernel_size - 1) / 2)\n",
    "\t\tself.location_conv = ConvNorm(2, attention_n_filters,\n",
    "\t\t\t\t\t\t\t\t\t  kernel_size=attention_kernel_size,\n",
    "\t\t\t\t\t\t\t\t\t  padding=padding, bias=False, stride=1,\n",
    "\t\t\t\t\t\t\t\t\t  dilation=1)\n",
    "\t\tself.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
    "\t\t\t\t\t\t\t\t\t\t bias=False, w_init_gain='tanh')\n",
    "\n",
    "\tdef forward(self, attention_weights_cat):\n",
    "\t\tprocessed_attention = self.location_conv(attention_weights_cat)\n",
    "\t\tprocessed_attention = processed_attention.transpose(1, 2)\n",
    "\t\tprocessed_attention = self.location_dense(processed_attention)\n",
    "\t\treturn processed_attention\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "\t\t\t\t attention_location_n_filters, attention_location_kernel_size):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\t\tself.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
    "\t\t\t\t\t\t\t\t\t  bias=False, w_init_gain='tanh')\n",
    "\t\tself.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
    "\t\t\t\t\t\t\t\t\t   w_init_gain='tanh')\n",
    "\t\tself.v = LinearNorm(attention_dim, 1, bias=False)\n",
    "\t\tself.location_layer = LocationLayer(attention_location_n_filters,\n",
    "\t\t\t\t\t\t\t\t\t\t\tattention_location_kernel_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tattention_dim)\n",
    "\t\tself.score_mask_value = -float('inf')\n",
    "\n",
    "\tdef get_alignment_energies(self, query, processed_memory,\n",
    "\t\t\t\t\t\t\t   attention_weights_cat):\n",
    "\t\t'''\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tquery: decoder output (batch, num_mels * n_frames_per_step)\n",
    "\t\tprocessed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
    "\t\tattention_weights_cat: cumulative and prev. att we;[ights (B, 2, max_time)\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\talignment (batch, max_time)\n",
    "\t\t'''\n",
    "\n",
    "\t\tprocessed_query = self.query_layer(query.unsqueeze(1))\n",
    "\t\tprocessed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "\t\tenergies = self.v(torch.tanh(\n",
    "\t\t\tprocessed_query + processed_attention_weights + processed_memory))\n",
    "\n",
    "\t\tenergies = energies.squeeze(-1)\n",
    "\t\treturn energies\n",
    "\n",
    "\tdef forward(self, attention_hidden_state, memory, processed_memory,\n",
    "\t\t\t\tattention_weights_cat, mask):\n",
    "\t\t'''\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tattention_hidden_state: attention rnn last output\n",
    "\t\tmemory: encoder outputs\n",
    "\t\tprocessed_memory: processed encoder outputs\n",
    "\t\tattention_weights_cat: previous and cummulative attention weights\n",
    "\t\tmask: binary mask for padded data\n",
    "\t\t'''\n",
    "\t\talignment = self.get_alignment_energies(\n",
    "\t\t\tattention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\talignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "\t\tattention_weights = F.softmax(alignment, dim=1)\n",
    "\t\tattention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "\t\tattention_context = attention_context.squeeze(1)\n",
    "\n",
    "\t\treturn attention_context, attention_weights\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "\tdef __init__(self, in_dim, sizes):\n",
    "\t\tsuper(Prenet, self).__init__()\n",
    "\t\tin_sizes = [in_dim] + sizes[:-1]\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[LinearNorm(in_size, out_size, bias=False)\n",
    "\t\t\t for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor linear in self.layers:\n",
    "\t\t\tx = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "\t'''Postnet\n",
    "\t\t- Five 1-d convolution with 512 channels and kernel size 5\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Postnet, self).__init__()\n",
    "\t\tself.convolutions = nn.ModuleList()\n",
    "\n",
    "\t\tself.convolutions.append(\n",
    "\t\t\tnn.Sequential(\n",
    "\t\t\t\tConvNorm(hps.num_mels, hps.postnet_embedding_dim,\n",
    "\t\t\t\t\t\t kernel_size=hps.postnet_kernel_size, stride=1,\n",
    "\t\t\t\t\t\t padding=int((hps.postnet_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t dilation=1, w_init_gain='tanh'),\n",
    "\t\t\t\tnn.BatchNorm1d(hps.postnet_embedding_dim))\n",
    "\t\t)\n",
    "\n",
    "\t\tfor i in range(1, hps.postnet_n_convolutions - 1):\n",
    "\t\t\tself.convolutions.append(\n",
    "\t\t\t\tnn.Sequential(\n",
    "\t\t\t\t\tConvNorm(hps.postnet_embedding_dim,\n",
    "\t\t\t\t\t\t\t hps.postnet_embedding_dim,\n",
    "\t\t\t\t\t\t\t kernel_size=hps.postnet_kernel_size, stride=1,\n",
    "\t\t\t\t\t\t\t padding=int((hps.postnet_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t dilation=1, w_init_gain='tanh'),\n",
    "\t\t\t\t\tnn.BatchNorm1d(hps.postnet_embedding_dim))\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.convolutions.append(\n",
    "\t\t\tnn.Sequential(\n",
    "\t\t\t\tConvNorm(hps.postnet_embedding_dim, hps.num_mels,\n",
    "\t\t\t\t\t\t kernel_size=hps.postnet_kernel_size, stride=1,\n",
    "\t\t\t\t\t\t padding=int((hps.postnet_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t dilation=1, w_init_gain='linear'),\n",
    "\t\t\t\tnn.BatchNorm1d(hps.num_mels))\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor i in range(len(self.convolutions) - 1):\n",
    "\t\t\tx = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
    "\t\tx = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\t'''Encoder module:\n",
    "\t\t- Three 1-d convolution banks\n",
    "\t\t- Bidirectional LSTM\n",
    "\t'''\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\n",
    "\t\tconvolutions = []\n",
    "\t\tfor _ in range(hps.encoder_n_convolutions):\n",
    "\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\tConvNorm(hps.encoder_embedding_dim,\n",
    "\t\t\t\t\t\t hps.encoder_embedding_dim,\n",
    "\t\t\t\t\t\t kernel_size=hps.encoder_kernel_size, stride=1,\n",
    "\t\t\t\t\t\t padding=int((hps.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t dilation=1, w_init_gain='relu'),\n",
    "\t\t\t\tnn.BatchNorm1d(hps.encoder_embedding_dim))\n",
    "\t\t\tconvolutions.append(conv_layer)\n",
    "\t\tself.convolutions = nn.ModuleList(convolutions)\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(hps.encoder_embedding_dim,\n",
    "\t\t\t\t\t\t\tint(hps.encoder_embedding_dim / 2), 1,\n",
    "\t\t\t\t\t\t\tbatch_first=True, bidirectional=True)\n",
    "\n",
    "\tdef forward(self, x, input_lengths):\n",
    "\t\tfor conv in self.convolutions:\n",
    "\t\t\tx = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
    "\n",
    "\t\tx = x.transpose(1, 2)\n",
    "\n",
    "\t\t# pytorch tensor are not reversible, hence the conversion\n",
    "\t\tinput_lengths = input_lengths.cpu().numpy()\n",
    "\t\tx = nn.utils.rnn.pack_padded_sequence(\n",
    "\t\t\tx, input_lengths, batch_first=True)\n",
    "\n",
    "\t\tself.lstm.flatten_parameters()\n",
    "\t\toutputs, _ = self.lstm(x)\n",
    "\n",
    "\t\toutputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "\t\t\toutputs, batch_first=True)\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef inference(self, x):\n",
    "\t\tfor conv in self.convolutions:\n",
    "\t\t\tx = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
    "\n",
    "\t\tx = x.transpose(1, 2)\n",
    "\n",
    "\t\tself.lstm.flatten_parameters()\n",
    "\t\toutputs, _ = self.lstm(x)\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "class Encoder3D(nn.Module):\n",
    "\t\"\"\"Encoder module:\n",
    "        - Three 3-d convolution banks\n",
    "        - Bidirectional LSTM\n",
    "    \"\"\"\n",
    "\n",
    "\tdef __init__(self, hparams):\n",
    "\t\tsuper(Encoder3D, self).__init__()\n",
    "\n",
    "\t\tself.hparams = hparams\n",
    "\t\tself.out_channel = hps.num_init_filters\n",
    "\t\tself.in_channel = 3\n",
    "\t\tconvolutions = []\n",
    "\n",
    "\t\tfor i in range(hps.encoder_n_convolutions):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.in_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t\tkernel_size=5, stride=(1, 2, 2),\n",
    "\t\t\t\t\t\t\t\t# padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t\tdilation=1, w_init_gain='relu'),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True)\n",
    "\t\t\t\t)\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\t\t\telse:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.in_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=(1, 2, 2),\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu'),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True)\n",
    "\t\t\t\t)\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\n",
    "\t\t\tif i == hps.encoder_n_convolutions - 1:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=(1, 3, 3),\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu'))\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\n",
    "\t\t\tself.in_channel = self.out_channel\n",
    "\t\t\tself.out_channel *= 2\n",
    "\t\tself.convolutions = nn.ModuleList(convolutions)\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
    "\t\t                    int(hparams.encoder_embedding_dim / 2), 1,\n",
    "\t\t                    batch_first=True, bidirectional=True)\n",
    "\n",
    "\tdef forward(self, x, input_lengths):\n",
    "\t\tfor conv in self.convolutions:\n",
    "\t\t\tx = F.dropout(conv(x), 0.5, self.training)\n",
    "\t\t# for i in range(len(self.convolutions)):\n",
    "\t\t# \tif i==0 or i==1 or i ==2:\n",
    "\t\t# \t\twith torch.no_grad():\n",
    "\t\t# \t\t\tx = F.dropout(self.convolutions[i](x), 0.5, self.training)\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tx = F.dropout(self.convolutions[i](x), 0.5, self.training)\n",
    "\n",
    "\t\tx = x.permute(0, 2, 1, 3, 4).squeeze(4).squeeze(3).contiguous()  # [bs x 90 x encoder_embedding_dim]\n",
    "\t\tprint(x.size())\n",
    "\t\t# pytorch tensor are not reversible, hence the conversion\n",
    "\t\tinput_lengths = input_lengths.cpu().numpy()\n",
    "\t\t# x = nn.utils.rnn.pack_padded_sequence(\n",
    "\t\t# \tx, input_lengths, batch_first=True)\n",
    "\n",
    "\t\t# self.lstm.flatten_parameters()\n",
    "\t\toutputs, _ = self.lstm(x)\n",
    "\t\t# print('outputs',outputs.size())\n",
    "\t\t# outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "\t\t# \toutputs, batch_first=True)\n",
    "\t\t# print('outputs', outputs.size())\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef inference(self, x):\n",
    "\t\tfor conv in self.convolutions:\n",
    "\t\t\tx = F.dropout(conv(x), 0.5, self.training)\n",
    "\n",
    "\t\tx = x.permute(0, 2, 1, 3, 4).squeeze(4).squeeze(3).contiguous()\n",
    "\t\t# self.lstm.flatten_parameters()\n",
    "\t\toutputs, _ = self.lstm(x)\t#x:B,T,C\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.num_mels = hps.num_mels\n",
    "\t\tself.n_frames_per_step = hps.n_frames_per_step\n",
    "\t\tself.encoder_embedding_dim = hps.encoder_embedding_dim\n",
    "\t\tself.attention_rnn_dim = hps.attention_rnn_dim\n",
    "\t\tself.decoder_rnn_dim = hps.decoder_rnn_dim\n",
    "\t\tself.prenet_dim = hps.prenet_dim\n",
    "\t\tself.max_decoder_steps = hps.max_decoder_steps\n",
    "\t\tself.gate_threshold = hps.gate_threshold\n",
    "\t\tself.p_attention_dropout = hps.p_attention_dropout\n",
    "\t\tself.p_decoder_dropout = hps.p_decoder_dropout\n",
    "\n",
    "\t\tself.prenet = Prenet(\n",
    "\t\t\thps.num_mels * hps.n_frames_per_step,\n",
    "\t\t\t[hps.prenet_dim, hps.prenet_dim])\n",
    "\n",
    "\t\tself.attention_rnn = nn.LSTMCell(\n",
    "\t\t\thps.prenet_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.attention_rnn_dim)\n",
    "\n",
    "\t\tself.attention_layer = Attention(\n",
    "\t\t\thps.attention_rnn_dim, hps.encoder_embedding_dim,\n",
    "\t\t\thps.attention_dim, hps.attention_location_n_filters,\n",
    "\t\t\thps.attention_location_kernel_size)\n",
    "\n",
    "\t\tself.decoder_rnn = nn.LSTMCell(\n",
    "\t\t\thps.attention_rnn_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.decoder_rnn_dim, 1)\n",
    "\n",
    "\t\tself.linear_projection = LinearNorm(\n",
    "\t\t\thps.decoder_rnn_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.num_mels * hps.n_frames_per_step)\n",
    "\n",
    "\t\tself.gate_layer = LinearNorm(\n",
    "\t\t\thps.decoder_rnn_dim + hps.encoder_embedding_dim, 1,\n",
    "\t\t\tbias=True, w_init_gain='sigmoid')\n",
    "\n",
    "\tdef get_go_frame(self, memory):\n",
    "\t\t''' Gets all zeros frames to use as first decoder input\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: decoder outputs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tdecoder_input: all zeros frames\n",
    "\t\t'''\n",
    "\t\tB = memory.size(0)\n",
    "\t\tdecoder_input = Variable(memory.data.new(\n",
    "\t\t\tB, self.num_mels * self.n_frames_per_step).zero_())\n",
    "\t\t# print(decoder_input)\n",
    "\t\t# print(decoder_input.size())\n",
    "\t\treturn decoder_input\n",
    "\n",
    "\tdef initialize_decoder_states(self, memory, mask):\n",
    "\t\t''' Initializes attention rnn states, decoder rnn states, attention\n",
    "\t\tweights, attention cumulative weights, attention context, stores memory\n",
    "\t\tand stores processed memory\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\t\tmask: Mask for padded data if training, expects None for inference\n",
    "\t\t'''\n",
    "\t\tB = memory.size(0)\n",
    "\t\tMAX_TIME = memory.size(1)\n",
    "\n",
    "\t\tself.attention_hidden = Variable(memory.data.new(\n",
    "\t\t\tB, self.attention_rnn_dim).zero_())\n",
    "\t\tself.attention_cell = Variable(memory.data.new(\n",
    "\t\t\tB, self.attention_rnn_dim).zero_())\n",
    "\n",
    "\t\tself.decoder_hidden = Variable(memory.data.new(\n",
    "\t\t\tB, self.decoder_rnn_dim).zero_())\n",
    "\t\tself.decoder_cell = Variable(memory.data.new(\n",
    "\t\t\tB, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "\t\tself.attention_weights = Variable(memory.data.new(\n",
    "\t\t\tB, MAX_TIME).zero_())\n",
    "\t\tself.attention_weights_cum = Variable(memory.data.new(\n",
    "\t\t\tB, MAX_TIME).zero_())\n",
    "\t\tself.attention_context = Variable(memory.data.new(\n",
    "\t\t\tB, self.encoder_embedding_dim).zero_())\n",
    "\n",
    "\t\tself.memory = memory\n",
    "\t\tself.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "\t\tself.mask = mask\n",
    "\n",
    "\tdef parse_decoder_inputs(self, decoder_inputs):\n",
    "\t\t''' Prepares decoder inputs, i.e. mel outputs\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tdecoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tinputs: processed decoder inputs\n",
    "\n",
    "\t\t'''\n",
    "\t\t# (B, num_mels, T_out) -> (B, T_out, num_mels)\n",
    "\t\tdecoder_inputs = decoder_inputs.transpose(1, 2).contiguous()\n",
    "\t\tdecoder_inputs = decoder_inputs.view(\n",
    "\t\t\tdecoder_inputs.size(0),\n",
    "\t\t\tint(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
    "\t\t# (B, T_out, num_mels) -> (T_out, B, num_mels)\n",
    "\t\tdecoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "\t\treturn decoder_inputs\n",
    "\n",
    "\tdef parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "\t\t''' Prepares decoder outputs for output\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmel_outputs:\n",
    "\t\tgate_outputs: gate output energies\n",
    "\t\talignments:\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs:\n",
    "\t\tgate_outpust: gate output energies\n",
    "\t\talignments:\n",
    "\t\t'''\n",
    "\t\t# (T_out, B) -> (B, T_out)\n",
    "\t\talignments = torch.stack(alignments).transpose(0, 1)\n",
    "\t\t# (T_out, B) -> (B, T_out)\n",
    "\n",
    "\t\tgate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "\t\tgate_outputs = gate_outputs.contiguous()\n",
    "\t\t# (T_out, B, num_mels) -> (B, T_out, num_mels)\n",
    "\t\tmel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "\t\t# decouple frames per step\n",
    "\t\tmel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.num_mels)\n",
    "\t\t# (B, T_out, num_mels) -> (B, num_mels, T_out)\n",
    "\t\tmel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n",
    "\n",
    "\tdef decode(self, decoder_input):\n",
    "\t\t''' Decoder step using stored states, attention and memory\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tdecoder_input: previous mel output\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_output:\n",
    "\t\tgate_output: gate output energies\n",
    "\t\tattention_weights:\n",
    "\t\t'''\n",
    "\t\tcell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "\n",
    "\t\tself.attention_hidden, self.attention_cell = self.attention_rnn(cell_input, (self.attention_hidden, self.attention_cell))\n",
    "\n",
    "\t\tself.attention_hidden = F.dropout(self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "\n",
    "\t\tattention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "\n",
    "\t\tself.attention_context, self.attention_weights = self.attention_layer(self.attention_hidden, self.memory, self.processed_memory, attention_weights_cat, self.mask)\n",
    "\n",
    "\t\tself.attention_weights_cum += self.attention_weights\n",
    "\n",
    "\t\tdecoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "\n",
    "\t\tself.decoder_hidden, self.decoder_cell = self.decoder_rnn(decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "\t\tself.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "\n",
    "\t\tdecoder_hidden_attention_context = torch.cat((self.decoder_hidden, self.attention_context), dim=1)\n",
    "\n",
    "\t\tdecoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "\t\tgate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "\n",
    "\t\treturn decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "\tdef forward(self, memory, decoder_inputs, memory_lengths):\n",
    "\t\t''' Decoder forward pass for training\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\t\tdecoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "\t\tmemory_lengths: Encoder output lengths for attention masking.\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs: mel outputs from the decoder\n",
    "\t\tgate_outputs: gate outputs from the decoder\n",
    "\t\talignments: sequence of attention weights from the decoder\n",
    "\t\t'''\n",
    "\t\t# print('Encoder outputs', memory.size())\n",
    "\t\tdecoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "\t\tdecoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "\t\tdecoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "\t\tdecoder_inputs = self.prenet(decoder_inputs)\n",
    "\t\t# print('decoder_input', decoder_input.size())\n",
    "\n",
    "\t\tself.initialize_decoder_states(\n",
    "\t\t\tmemory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "\t\tmel_outputs, gate_outputs, alignments = [], [], []\n",
    "\t\twhile len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
    "\t\t\tdecoder_input = decoder_inputs[len(mel_outputs)]\n",
    "\t\t\tmel_output, gate_output, attention_weights = self.decode(\n",
    "\t\t\t\tdecoder_input)\n",
    "\t\t\t# print('mel_output', mel_output.size())\n",
    "\t\t\t# print('gate_output', gate_output.size())\n",
    "\t\t\t# print('attention_weights', attention_weights.size())\n",
    "\t\t\tmel_outputs += [mel_output.squeeze(1)]\n",
    "\t\t\tgate_outputs += [gate_output.squeeze((1))]\n",
    "\t\t\talignments += [attention_weights]\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "\t\t\tmel_outputs, gate_outputs, alignments)\n",
    "\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n",
    "\n",
    "\tdef inference(self, memory):\n",
    "\t\t''' Decoder inference\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs: mel outputs from the decoder\n",
    "\t\tgate_outputs: gate outputs from the decoder\n",
    "\t\talignments: sequence of attention weights from the decoder\n",
    "\t\t'''\n",
    "\t\tdecoder_input = self.get_go_frame(memory)\n",
    "\n",
    "\t\tself.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = [], [], []\n",
    "\t\twhile True:\n",
    "\t\t\tdecoder_input = self.prenet(decoder_input)\n",
    "\t\t\tmel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "\t\t\tmel_outputs += [mel_output.squeeze(1)]\n",
    "\t\t\tgate_outputs += [gate_output]\n",
    "\t\t\talignments += [alignment]\n",
    "\n",
    "\t\t\tif sum(torch.sigmoid(gate_output.data))/len(gate_output.data) > self.gate_threshold:\n",
    "\t\t\t\tprint('Terminated by gate.')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t# elif len(mel_outputs) > 1 and is_end_of_frames(mel_output):\n",
    "\t\t\t# \tprint('Warning: End with low power.')\n",
    "\t\t\t# \tbreak\n",
    "\t\t\telif len(mel_outputs) == self.max_decoder_steps:\n",
    "\t\t\t\tprint('Warning: Reached max decoder steps.')\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tdecoder_input = mel_output\n",
    "\n",
    "\t\tprint(mel_outputs.shape)\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "\t\t\tmel_outputs, gate_outputs, alignments)\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n",
    "\n",
    "def is_end_of_frames(output, eps = 0.2):\n",
    "    return (output.data <= eps).all()\n",
    "\n",
    "class Tacotron2(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Tacotron2, self).__init__()\n",
    "\t\tself.num_mels = hps.num_mels\n",
    "\t\tself.mask_padding = hps.mask_padding\n",
    "\t\tself.n_frames_per_step = hps.n_frames_per_step\n",
    "\t\tself.embedding = nn.Embedding(\n",
    "\t\t\thps.n_symbols, hps.symbols_embedding_dim)\n",
    "\t\tstd = sqrt(2.0/(hps.n_symbols+hps.symbols_embedding_dim))\n",
    "\t\tval = sqrt(3.0)*std  # uniform bounds for std\n",
    "\t\tself.embedding.weight.data.uniform_(-val, val)\n",
    "\t\t# self.encoder = Encoder()\n",
    "\t\tself.encoder = Encoder3D(hps)\n",
    "\t\tself.decoder = Decoder()\n",
    "\t\tself.postnet = Postnet()\n",
    "\n",
    "\tdef parse_batch(self, batch):\n",
    "\t\ttext_padded, input_lengths, mel_padded, gate_padded, output_lengths = batch\n",
    "\t\ttext_padded = to_var(text_padded).long()\n",
    "\t\tinput_lengths = to_var(input_lengths).long()\n",
    "\t\tmax_len = torch.max(input_lengths.data).item()\n",
    "\t\tmel_padded = to_var(mel_padded).float()\n",
    "\t\tgate_padded = to_var(gate_padded).float()\n",
    "\t\toutput_lengths = to_var(output_lengths).long()\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\t(text_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
    "\t\t\t(mel_padded, gate_padded))\n",
    "\n",
    "\tdef parse_batch_vid(self, batch):\n",
    "\t\tvid_padded, input_lengths, mel_padded, gate_padded, target_lengths, split_infos, embed_targets = batch\n",
    "\t\tvid_padded = to_var(vid_padded).float()\n",
    "\t\tinput_lengths = to_var(input_lengths).float()\n",
    "\t\tmel_padded = to_var(mel_padded).float()\n",
    "\t\tgate_padded = to_var(gate_padded).float()\n",
    "\t\ttarget_lengths = to_var(target_lengths).float()\n",
    "\n",
    "\t\tmax_len_vid = split_infos[0].data.item()\n",
    "\t\tmax_len_target = split_infos[1].data.item()\n",
    "\n",
    "\n",
    "\t\tmel_padded = to_var(mel_padded).float()\n",
    "\n",
    "\t\treturn(\n",
    "\t\t\t(vid_padded, input_lengths, mel_padded, max_len_vid, target_lengths),\n",
    "\t\t\t(mel_padded, gate_padded))\n",
    "\n",
    "\tdef parse_output(self, outputs, output_lengths=None):\n",
    "\t\tif self.mask_padding and output_lengths is not None:\n",
    "\t\t\tmask = ~get_mask_from_lengths(output_lengths, True) # (B, T)\n",
    "\t\t\tmask = mask.expand(self.num_mels, mask.size(0), mask.size(1)) # (80, B, T)\n",
    "\t\t\tmask = mask.permute(1, 0, 2) # (B, 80, T)\n",
    "\t\t\t\n",
    "\t\t\toutputs[0].data.masked_fill_(mask, 0.0) # (B, 80, T)\n",
    "\t\t\toutputs[1].data.masked_fill_(mask, 0.0) # (B, 80, T)\n",
    "\t\t\tslice = torch.arange(0, mask.size(2), self.n_frames_per_step)\n",
    "\t\t\toutputs[2].data.masked_fill_(mask[:, 0, slice], 1e3)  # gate energies (B, T//n_frames_per_step)\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tvid_inputs = inputs[\"video\"].transpose(-3, -4)\n",
    "\t\tmels = inputs[\"mel\"]\n",
    "\t\toutput_lengths = torch.tensor([mels.shape[-1]])\n",
    "\t\tvid_lengths = torch.tensor([vid_inputs.shape[-3]])\n",
    "  \n",
    "  \n",
    "\t\tvid_lengths, output_lengths = vid_lengths.data, output_lengths.data\n",
    "\n",
    "\t\tembedded_inputs = vid_inputs\n",
    "\t\t# print('vid_inputs',vid_inputs)\n",
    "\n",
    "\t\tencoder_outputs = self.encoder(embedded_inputs, vid_lengths)\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.decoder(encoder_outputs, mels, memory_lengths=vid_lengths)\n",
    "\n",
    "\t\tmel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\t\tmel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "\t\tmel_outputs, mel_outputs_postnet, gate_outputs, alignments = self.parse_output(\n",
    "\t\t\t[mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
    "\t\t\toutput_lengths)\n",
    "\n",
    "\t\treturn mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
    "\n",
    "\tdef inference(self, inputs):\n",
    "\t\tvid_inputs = inputs[\"video\"].transpose(-3, -4)\n",
    "\t\n",
    "\t\tembedded_inputs = vid_inputs\n",
    "\n",
    "\t\tencoder_outputs = self.encoder.inference(embedded_inputs)\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
    "\t\t\tencoder_outputs)\n",
    "\t\tmel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\n",
    "\t\tmel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\t\tmel_outputs, mel_outputs_postnet, gate_outputs, alignments = self.parse_output(\n",
    "\t\t\t[mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
    "\n",
    "\t\treturn mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
    "\n",
    "\tdef teacher_infer(self, inputs, mels):\n",
    "\t\til, _ =  torch.sort(torch.LongTensor([len(x) for x in inputs]),\n",
    "\t\t\t\t\t\t\tdim = 0, descending = True)\n",
    "\t\tvid_lengths = to_var(il)\n",
    "\n",
    "\t\tembedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
    "\n",
    "\t\tencoder_outputs = self.encoder(embedded_inputs, vid_lengths)\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.decoder(\n",
    "\t\t\tencoder_outputs, mels, memory_lengths=vid_lengths)\n",
    "\t\t\n",
    "\t\tmel_outputs_postnet = self.postnet(mel_outputs)\n",
    "\t\tmel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "\n",
    "\t\treturn self.parse_output(\n",
    "\t\t\t[mel_outputs, mel_outputs_postnet, gate_outputs, alignments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Tacotron2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_inputs = torch.randn(1, 75, 3, 48, 48)\n",
    "mels = torch.randn(1, 80, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75, 384])\n"
     ]
    }
   ],
   "source": [
    "mel_outputs, mel_outputs_postnet, gate_outputs, alignments = self({\"video\": vid_inputs, \"mel\": mels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 150])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_outputs_postnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepcasa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
