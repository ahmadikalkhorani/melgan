{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvNorm3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear', activation=torch.nn.ReLU, residual=False, ):\n",
    "        super(ConvNorm3D, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.residual = residual\n",
    "        self.conv3d = torch.nn.Conv3d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "        self.batched = torch.nn.BatchNorm3d(out_channels)\n",
    "        self.activation = activation()\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv3d.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.batched.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.activation.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        # pdb.set_trace()\n",
    "        conv_signal = self.conv3d(signal)\n",
    "\n",
    "        batched = self.batched(conv_signal)\n",
    "\n",
    "        if self.residual:\n",
    "            batched = batched + signal\n",
    "        activated = self.activation(batched)\n",
    "\n",
    "        return activated\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class Encoder3D(nn.Module):\n",
    "\t\"\"\"Encoder module:\n",
    "        - Three 3-d convolution banks\n",
    "        - Bidirectional LSTM\n",
    "    \"\"\"\n",
    "\n",
    "\tdef __init__(self, num_out_feat = 80, encoder_embedding_dim = 384, duration = 3.0, fps = 25, sr = 16000, encoder_n_convolutions = 5, num_init_filters= 24):\n",
    "\t\tsuper(Encoder3D, self).__init__()\n",
    "  \n",
    "\t\tT = int(duration * fps)\n",
    "\n",
    "\t\tself.out_channel = num_init_filters\n",
    "\t\tself.in_channel = 3\n",
    "\t\tconvolutions = []\n",
    "  \n",
    "\t\tself.resize = torchvision.transforms.Resize((96, 96))\n",
    "\n",
    "\t\tfor i in range(encoder_n_convolutions):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.in_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t\tkernel_size=5, stride=(1, 2, 2),\n",
    "\t\t\t\t\t\t\t\t# padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t\tdilation=1, w_init_gain='relu'),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True)\n",
    "\t\t\t\t)\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\t\t\telse:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.in_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=(1, 2, 2),\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu'),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True),\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=1,\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu', residual=True)\n",
    "\t\t\t\t)\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\n",
    "\t\t\tif i == encoder_n_convolutions - 1:\n",
    "\t\t\t\tconv_layer = nn.Sequential(\n",
    "\t\t\t\t\tConvNorm3D(self.out_channel, self.out_channel,\n",
    "\t\t\t\t\t\t\t   kernel_size=3, stride=(1, 3, 3),\n",
    "\t\t\t\t\t\t\t   # padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
    "\t\t\t\t\t\t\t   dilation=1, w_init_gain='relu'), \n",
    "     \n",
    "     )\n",
    "\t\t\t\tconvolutions.append(conv_layer)\n",
    "\n",
    "\t\t\tself.in_channel = self.out_channel\n",
    "\t\t\tself.out_channel *= 2\n",
    "\t\tself.convolutions = nn.ModuleList(convolutions)\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(encoder_embedding_dim,\n",
    "\t\t                    num_out_feat , 1,\n",
    "\t\t                    batch_first=True, bidirectional=True)\n",
    "\t\tself.fc = nn.Linear(2*num_out_feat , num_out_feat )\n",
    "\t\tself.conv_out = nn.Sequential(\n",
    "      \t\t\tnn.ConvTranspose1d(T, int(((duration*sr)*22050/sr)/256) , kernel_size=1), \n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x, input_lengths = None):\n",
    "\t\tB, T, C, H, W = x.size() \n",
    "\t\tx = x.reshape(B*T, C, H, W)\n",
    "\t\tx = self.resize(x).reshape(B, T, C, 96, 96)\n",
    "\t\tfor conv in self.convolutions:\n",
    "\t\t\tx = F.dropout(conv(x), 0.5, self.training)\n",
    "\t\t# for i in range(len(self.convolutions)):\n",
    "\t\t# \tif i==0 or i==1 or i ==2:\n",
    "\t\t# \t\twith torch.no_grad():\n",
    "\t\t# \t\t\tx = F.dropout(self.convolutions[i](x), 0.5, self.training)\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tx = F.dropout(self.convolutions[i](x), 0.5, self.training)\n",
    "\n",
    "\t\tx = x.permute(0, 2, 1, 3, 4).squeeze(4).squeeze(3).contiguous()  # [bs x 90 x encoder_embedding_dim]\n",
    "\t\t# print(x.size())\n",
    "\t\t# pytorch tensor are not reversible, hence the conversion\n",
    "\t\t# input_lengths = input_lengths.cpu().numpy()\n",
    "\t\t# x = nn.utils.rnn.pack_padded_sequence(\n",
    "\t\t# \tx, input_lengths, batch_first=True)\n",
    "\n",
    "\t\t# self.lstm.flatten_parameters()\n",
    "\t\toutputs, _ = self.lstm(x)\n",
    "\t\toutputs = self.fc(outputs)\n",
    "\t\toutputs = self.conv_out (outputs)\n",
    "\n",
    "\n",
    "\t\treturn outputs.transpose(-1, -2).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text import symbols\n",
    "\n",
    "\n",
    "class hparams:\n",
    "\t################################\n",
    "\t# Data Parameters              #\n",
    "\t################################\n",
    "\ttext_cleaners=['english_cleaners']\n",
    "\n",
    "\t################################\n",
    "\t# Audio                        #\n",
    "\t################################\n",
    "\tnum_mels = 80\n",
    "\tnum_freq = 1025\n",
    "\tsample_rate = 16000\n",
    "\tframe_length_ms = 50\n",
    "\tframe_shift_ms = 12.5\n",
    "\tpreemphasis = 0.97\n",
    "\tmin_level_db = -100\n",
    "\tref_level_db = 20\n",
    "\tpower = 1.5\n",
    "\tgl_iters = 100\n",
    "\n",
    "\t################################\n",
    "\t# Model Parameters             #\n",
    "\t################################\n",
    "\t# n_symbols = len(symbols)\n",
    "\tsymbols_embedding_dim = 512\n",
    "\n",
    "\t# Encoder parameters\n",
    "\tencoder_kernel_size = 5\n",
    "\n",
    "\t# Decoder parameters\n",
    "\tn_frames_per_step = 2\n",
    "\tdecoder_rnn_dim = 1024\n",
    "\tprenet_dim = 256\n",
    "\tmax_decoder_steps = 120\n",
    "\tgate_threshold = 0.5\n",
    "\tp_attention_dropout = 0.1\n",
    "\tp_decoder_dropout = 0.1\n",
    "\n",
    "\t# Attention parameters\n",
    "\tattention_rnn_dim = 1024\n",
    "\tattention_dim = 128\n",
    "\n",
    "\t# Location Layer parameters\n",
    "\tattention_location_n_filters = 32\n",
    "\tattention_location_kernel_size = 31\n",
    "\n",
    "\t# Mel-post processing network parameters\n",
    "\tpostnet_embedding_dim = 512\n",
    "\tpostnet_kernel_size = 5\n",
    "\tpostnet_n_convolutions = 5\n",
    "\n",
    "\t################################\n",
    "\t# Train                        #\n",
    "\t################################\n",
    "\tis_cuda = True\n",
    "\tpin_mem = True\n",
    "\tn_workers = 8\n",
    "\tlr = 2e-3\n",
    "\tbetas = (0.9, 0.999)\n",
    "\teps = 1e-6\n",
    "\tsch = True\n",
    "\tsch_step = 4000\n",
    "\tmax_iter = 1e6\n",
    "\tbatch_size = 40\n",
    "\titers_per_log = 50\n",
    "\titers_per_sample = 500\n",
    "\titers_per_ckpt = 1000\n",
    "\tweight_decay = 1e-6\n",
    "\tgrad_clip_thresh = 1.0\n",
    "\tmask_padding = True\n",
    "\tp = 10 # mel spec loss penalty\n",
    "\teg_text = 'Make America great again!'\n",
    "\n",
    "\t############# added\n",
    "\tiscrop = True\n",
    "\tencoder_embedding_dim = 384  # encoder_lstm_units\n",
    "\tencoder_n_convolutions = 5  # enc_conv_num_blocks\n",
    "\n",
    "\tnum_init_filters= 24\n",
    "\n",
    "\tprenet_layers= [256, 256]\n",
    "\tdecoder_layers= 2\n",
    "\tdecoder_lstm_units= 256\n",
    "\n",
    "\ttacotron_teacher_forcing_start_decay= 29000\n",
    "\ttacotron_teacher_forcing_decay_steps= 130000\n",
    "\n",
    "\tT= 90 #90\n",
    "\toverlap= 15\n",
    "\tmel_overlap= 40\n",
    "\tmel_step_size= 240\n",
    "\timg_size = 96\n",
    "\tfps= 30\n",
    "\n",
    "\n",
    "\tuse_lws = False\n",
    "\t# Mel spectrogram\n",
    "\tn_fft = 800  # Extra window size is filled with 0 paddings to match this parameter\n",
    "\thop_size = 200  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\n",
    "\twin_size = 800  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n",
    "\n",
    "\t# M-AILABS (and other datasets) trim params (these parameters are usually correct for any\n",
    "\t# data, but definitely must be tuned for specific speakers)\n",
    "\ttrim_fft_size = 512\n",
    "\ttrim_hop_size = 128\n",
    "\ttrim_top_db = 23\n",
    "\n",
    "\t# Mel and Linear spectrograms normalization/scaling and clipping\n",
    "\tsignal_normalization = True\n",
    "\t# Whether to normalize mel spectrograms to some predefined range (following below parameters)\n",
    "\tallow_clipping_in_normalization = True # Only relevant if mel_normalization = True\n",
    "\tsymmetric_mels = True\n",
    "\t# Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,\n",
    "\t# faster and cleaner convergence)\n",
    "\tmax_abs_value = 4.\n",
    "\t# max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not\n",
    "\t# be too big to avoid gradient explosion,\n",
    "\t# not too small for fast convergence)\n",
    "\tnormalize_for_wavenet = True\n",
    "\t# whether to rescale to [0, 1] for wavenet. (better audio quality)\n",
    "\tclip_for_wavenet = True\n",
    "\t# whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n",
    "\n",
    "\t# Contribution by @begeekmyfriend\n",
    "\t# Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude\n",
    "\t# levels. Also allows for better G&L phase reconstruction)\n",
    "\tpreemphasize = True # whether to apply filter\n",
    "\n",
    "\tfmin = 55\n",
    "\t# Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To\n",
    "\t# test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n",
    "\tfmax = 7600  # To be increased/reduced depending on data.\n",
    "\n",
    "\t# Griffin Lim\n",
    "\t# Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n",
    "\tgriffin_lim_iters = 60\n",
    "# Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n",
    "###########################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal\n",
    "\n",
    "class ConvNorm3D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear', activation=torch.nn.ReLU, residual=False):\n",
    "        super(ConvNorm3D, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.residual = residual\n",
    "        self.conv3d = torch.nn.Conv3d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "        self.batched = torch.nn.BatchNorm3d(out_channels)\n",
    "        self.activation = activation()\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv3d.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.batched.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "        # torch.nn.init.xavier_uniform_(\n",
    "        #     self.activation.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv3d(signal)\n",
    "\n",
    "        batched = self.batched(conv_signal)\n",
    "\n",
    "        if self.residual:\n",
    "            batched = batched + signal\n",
    "        activated = self.activation(batched)\n",
    "\n",
    "        return activated\n",
    "\n",
    "\n",
    "class LocationLayer(nn.Module):\n",
    "\tdef __init__(self, attention_n_filters, attention_kernel_size,\n",
    "\t\t\t\t attention_dim):\n",
    "\t\tsuper(LocationLayer, self).__init__()\n",
    "\t\tpadding = int((attention_kernel_size - 1) / 2)\n",
    "\t\tself.location_conv = ConvNorm(2, attention_n_filters,\n",
    "\t\t\t\t\t\t\t\t\t  kernel_size=attention_kernel_size,\n",
    "\t\t\t\t\t\t\t\t\t  padding=padding, bias=False, stride=1,\n",
    "\t\t\t\t\t\t\t\t\t  dilation=1)\n",
    "\t\tself.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
    "\t\t\t\t\t\t\t\t\t\t bias=False, w_init_gain='tanh')\n",
    "\n",
    "\tdef forward(self, attention_weights_cat):\n",
    "\t\tprocessed_attention = self.location_conv(attention_weights_cat)\n",
    "\t\tprocessed_attention = processed_attention.transpose(1, 2)\n",
    "\t\tprocessed_attention = self.location_dense(processed_attention)\n",
    "\t\treturn processed_attention\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "\t\t\t\t attention_location_n_filters, attention_location_kernel_size):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\t\tself.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
    "\t\t\t\t\t\t\t\t\t  bias=False, w_init_gain='tanh')\n",
    "\t\tself.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
    "\t\t\t\t\t\t\t\t\t   w_init_gain='tanh')\n",
    "\t\tself.v = LinearNorm(attention_dim, 1, bias=False)\n",
    "\t\tself.location_layer = LocationLayer(attention_location_n_filters,\n",
    "\t\t\t\t\t\t\t\t\t\t\tattention_location_kernel_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tattention_dim)\n",
    "\t\tself.score_mask_value = -float('inf')\n",
    "\n",
    "\tdef get_alignment_energies(self, query, processed_memory,\n",
    "\t\t\t\t\t\t\t   attention_weights_cat):\n",
    "\t\t'''\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tquery: decoder output (batch, num_mels * n_frames_per_step)\n",
    "\t\tprocessed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
    "\t\tattention_weights_cat: cumulative and prev. att we;[ights (B, 2, max_time)\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\talignment (batch, max_time)\n",
    "\t\t'''\n",
    "\n",
    "\t\tprocessed_query = self.query_layer(query.unsqueeze(1))\n",
    "\t\tprocessed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "\t\tenergies = self.v(torch.tanh(\n",
    "\t\t\tprocessed_query + processed_attention_weights + processed_memory))\n",
    "\n",
    "\t\tenergies = energies.squeeze(-1)\n",
    "\t\treturn energies\n",
    "\n",
    "\tdef forward(self, attention_hidden_state, memory, processed_memory,\n",
    "\t\t\t\tattention_weights_cat, mask):\n",
    "\t\t'''\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tattention_hidden_state: attention rnn last output\n",
    "\t\tmemory: encoder outputs\n",
    "\t\tprocessed_memory: processed encoder outputs\n",
    "\t\tattention_weights_cat: previous and cummulative attention weights\n",
    "\t\tmask: binary mask for padded data\n",
    "\t\t'''\n",
    "\t\talignment = self.get_alignment_energies(\n",
    "\t\t\tattention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\talignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "\t\tattention_weights = F.softmax(alignment, dim=1)\n",
    "\t\tattention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "\t\tattention_context = attention_context.squeeze(1)\n",
    "\n",
    "\t\treturn attention_context, attention_weights\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "\tdef __init__(self, in_dim, sizes):\n",
    "\t\tsuper(Prenet, self).__init__()\n",
    "\t\tin_sizes = [in_dim] + sizes[:-1]\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[LinearNorm(in_size, out_size, bias=False)\n",
    "\t\t\t for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor linear in self.layers:\n",
    "\t\t\tx = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.num_mels = hps.num_mels\n",
    "\t\tself.n_frames_per_step = hps.n_frames_per_step\n",
    "\t\tself.encoder_embedding_dim = hps.encoder_embedding_dim\n",
    "\t\tself.attention_rnn_dim = hps.attention_rnn_dim\n",
    "\t\tself.decoder_rnn_dim = hps.decoder_rnn_dim\n",
    "\t\tself.prenet_dim = hps.prenet_dim\n",
    "\t\tself.max_decoder_steps = hps.max_decoder_steps\n",
    "\t\tself.gate_threshold = hps.gate_threshold\n",
    "\t\tself.p_attention_dropout = hps.p_attention_dropout\n",
    "\t\tself.p_decoder_dropout = hps.p_decoder_dropout\n",
    "\n",
    "\t\tself.prenet = Prenet(\n",
    "\t\t\thps.num_mels * hps.n_frames_per_step,\n",
    "\t\t\t[hps.prenet_dim, hps.prenet_dim])\n",
    "\n",
    "\t\tself.attention_rnn = nn.LSTMCell(\n",
    "\t\t\thps.prenet_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.attention_rnn_dim)\n",
    "\n",
    "\t\tself.attention_layer = Attention(\n",
    "\t\t\thps.attention_rnn_dim, hps.encoder_embedding_dim,\n",
    "\t\t\thps.attention_dim, hps.attention_location_n_filters,\n",
    "\t\t\thps.attention_location_kernel_size)\n",
    "\n",
    "\t\tself.decoder_rnn = nn.LSTMCell(\n",
    "\t\t\thps.attention_rnn_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.decoder_rnn_dim, 1)\n",
    "\n",
    "\t\tself.linear_projection = LinearNorm(\n",
    "\t\t\thps.decoder_rnn_dim + hps.encoder_embedding_dim,\n",
    "\t\t\thps.num_mels * hps.n_frames_per_step)\n",
    "\n",
    "\t\tself.gate_layer = LinearNorm(\n",
    "\t\t\thps.decoder_rnn_dim + hps.encoder_embedding_dim, 1,\n",
    "\t\t\tbias=True, w_init_gain='sigmoid')\n",
    "\n",
    "\tdef get_go_frame(self, memory):\n",
    "\t\t''' Gets all zeros frames to use as first decoder input\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: decoder outputs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tdecoder_input: all zeros frames\n",
    "\t\t'''\n",
    "\t\tB = memory.size(0)\n",
    "\t\tdecoder_input = Variable(memory.data.new(\n",
    "\t\t\tB, self.num_mels * self.n_frames_per_step).zero_())\n",
    "\t\tprint(decoder_input)\n",
    "\t\tprint(decoder_input.size())\n",
    "\t\treturn decoder_input\n",
    "\n",
    "\tdef initialize_decoder_states(self, memory, mask):\n",
    "\t\t''' Initializes attention rnn states, decoder rnn states, attention\n",
    "\t\tweights, attention cumulative weights, attention context, stores memory\n",
    "\t\tand stores processed memory\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\t\tmask: Mask for padded data if training, expects None for inference\n",
    "\t\t'''\n",
    "\t\tB = memory.size(0)\n",
    "\t\tMAX_TIME = memory.size(1)\n",
    "\n",
    "\t\tself.attention_hidden = Variable(memory.data.new(\n",
    "\t\t\tB, self.attention_rnn_dim).zero_())\n",
    "\t\tself.attention_cell = Variable(memory.data.new(\n",
    "\t\t\tB, self.attention_rnn_dim).zero_())\n",
    "\n",
    "\t\tself.decoder_hidden = Variable(memory.data.new(\n",
    "\t\t\tB, self.decoder_rnn_dim).zero_())\n",
    "\t\tself.decoder_cell = Variable(memory.data.new(\n",
    "\t\t\tB, self.decoder_rnn_dim).zero_())\n",
    "\n",
    "\t\tself.attention_weights = Variable(memory.data.new(\n",
    "\t\t\tB, MAX_TIME).zero_())\n",
    "\t\tself.attention_weights_cum = Variable(memory.data.new(\n",
    "\t\t\tB, MAX_TIME).zero_())\n",
    "\t\tself.attention_context = Variable(memory.data.new(\n",
    "\t\t\tB, self.encoder_embedding_dim).zero_())\n",
    "\n",
    "\t\tself.memory = memory\n",
    "\t\tself.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "\t\tself.mask = mask\n",
    "\n",
    "\tdef parse_decoder_inputs(self, decoder_inputs):\n",
    "\t\t''' Prepares decoder inputs, i.e. mel outputs\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tdecoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tinputs: processed decoder inputs\n",
    "\n",
    "\t\t'''\n",
    "\t\t# (B, num_mels, T_out) -> (B, T_out, num_mels)\n",
    "\t\tdecoder_inputs = decoder_inputs.transpose(1, 2).contiguous()\n",
    "\t\tdecoder_inputs = decoder_inputs.view(\n",
    "\t\t\tdecoder_inputs.size(0),\n",
    "\t\t\tint(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
    "\t\t# (B, T_out, num_mels) -> (T_out, B, num_mels)\n",
    "\t\tdecoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "\t\treturn decoder_inputs\n",
    "\n",
    "\tdef parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "\t\t''' Prepares decoder outputs for output\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmel_outputs:\n",
    "\t\tgate_outputs: gate output energies\n",
    "\t\talignments:\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs:\n",
    "\t\tgate_outpust: gate output energies\n",
    "\t\talignments:\n",
    "\t\t'''\n",
    "\t\t# (T_out, B) -> (B, T_out)\n",
    "\t\talignments = torch.stack(alignments).transpose(0, 1)\n",
    "\t\t# (T_out, B) -> (B, T_out)\n",
    "\n",
    "\t\tgate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "\t\tgate_outputs = gate_outputs.contiguous()\n",
    "\t\t# (T_out, B, num_mels) -> (B, T_out, num_mels)\n",
    "\t\tmel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "\t\t# decouple frames per step\n",
    "\t\tmel_outputs = mel_outputs.view(\n",
    "\t\t\tmel_outputs.size(0), -1, self.num_mels)\n",
    "\t\t# (B, T_out, num_mels) -> (B, num_mels, T_out)\n",
    "\t\tmel_outputs = mel_outputs.transpose(1, 2)\n",
    "\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n",
    "\n",
    "\tdef decode(self, decoder_input):\n",
    "\t\t''' Decoder step using stored states, attention and memory\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tdecoder_input: previous mel output\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_output:\n",
    "\t\tgate_output: gate output energies\n",
    "\t\tattention_weights:\n",
    "\t\t'''\n",
    "\t\tcell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "\n",
    "\t\tself.attention_hidden, self.attention_cell = self.attention_rnn(cell_input, (self.attention_hidden, self.attention_cell))\n",
    "\n",
    "\t\tself.attention_hidden = F.dropout(self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "\n",
    "\t\tattention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "\n",
    "\t\tself.attention_context, self.attention_weights = self.attention_layer(self.attention_hidden, self.memory, self.processed_memory, attention_weights_cat, self.mask)\n",
    "\n",
    "\t\tself.attention_weights_cum += self.attention_weights\n",
    "\n",
    "\t\tdecoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "\n",
    "\t\tself.decoder_hidden, self.decoder_cell = self.decoder_rnn(decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "\t\tself.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "\n",
    "\t\tdecoder_hidden_attention_context = torch.cat((self.decoder_hidden, self.attention_context), dim=1)\n",
    "\n",
    "\t\tdecoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "\n",
    "\t\tgate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "\n",
    "\t\treturn decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "\tdef forward(self, memory, decoder_inputs, memory_lengths):\n",
    "\t\t''' Decoder forward pass for training\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\t\tdecoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
    "\t\tmemory_lengths: Encoder output lengths for attention masking.\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs: mel outputs from the decoder\n",
    "\t\tgate_outputs: gate outputs from the decoder\n",
    "\t\talignments: sequence of attention weights from the decoder\n",
    "\t\t'''\n",
    "\t\tprint('Encoder outputs', memory.size())\n",
    "\t\tdecoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "\t\tdecoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "\t\tdecoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "\t\tdecoder_inputs = self.prenet(decoder_inputs)\n",
    "\t\t# print('decoder_input', decoder_input.size())\n",
    "\n",
    "\t\tself.initialize_decoder_states(\n",
    "\t\t\tmemory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "\t\tmel_outputs, gate_outputs, alignments = [], [], []\n",
    "\t\twhile len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
    "\t\t\tdecoder_input = decoder_inputs[len(mel_outputs)]\n",
    "\t\t\tmel_output, gate_output, attention_weights = self.decode(\n",
    "\t\t\t\tdecoder_input)\n",
    "\t\t\tprint('mel_output', mel_output.size())\n",
    "\t\t\tprint('gate_output', gate_output.size())\n",
    "\t\t\tprint('attention_weights', attention_weights.size())\n",
    "\t\t\tmel_outputs += [mel_output.squeeze(1)]\n",
    "\t\t\tgate_outputs += [gate_output.squeeze()]\n",
    "\t\t\talignments += [attention_weights]\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "\t\t\tmel_outputs, gate_outputs, alignments)\n",
    "\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n",
    "\n",
    "\tdef inference(self, memory):\n",
    "\t\t''' Decoder inference\n",
    "\t\tPARAMS\n",
    "\t\t------\n",
    "\t\tmemory: Encoder outputs\n",
    "\n",
    "\t\tRETURNS\n",
    "\t\t-------\n",
    "\t\tmel_outputs: mel outputs from the decoder\n",
    "\t\tgate_outputs: gate outputs from the decoder\n",
    "\t\talignments: sequence of attention weights from the decoder\n",
    "\t\t'''\n",
    "\t\tdecoder_input = self.get_go_frame(memory)\n",
    "\n",
    "\t\tself.initialize_decoder_states(memory, mask=None)\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = [], [], []\n",
    "\t\twhile True:\n",
    "\t\t\tdecoder_input = self.prenet(decoder_input)\n",
    "\t\t\tmel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "\t\t\tmel_outputs += [mel_output.squeeze(1)]\n",
    "\t\t\tgate_outputs += [gate_output]\n",
    "\t\t\talignments += [alignment]\n",
    "\n",
    "\t\t\tif sum(torch.sigmoid(gate_output.data))/len(gate_output.data) > self.gate_threshold:\n",
    "\t\t\t\tprint('Terminated by gate.')\n",
    "\t\t\t\tbreak\n",
    "\t\t\t# elif len(mel_outputs) > 1 and is_end_of_frames(mel_output):\n",
    "\t\t\t# \tprint('Warning: End with low power.')\n",
    "\t\t\t# \tbreak\n",
    "\t\t\telif len(mel_outputs) == self.max_decoder_steps:\n",
    "\t\t\t\tprint('Warning: Reached max decoder steps.')\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tdecoder_input = mel_output\n",
    "\n",
    "\t\tmel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "\t\t\tmel_outputs, gate_outputs, alignments)\n",
    "\t\treturn mel_outputs, gate_outputs, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 258])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out = enc(torch.randn(1, 3, 75, 160, 160))\n",
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 160])\n",
      "Terminated by gate.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.inference(\n",
    "    torch.randn(1, 75, 384)\n",
    ")[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepcasa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
